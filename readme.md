# ğŸ§¬ BioMed RAG Chatbot (Gemma + FAISS)

## ğŸ“Œ Overview
The **BioMed RAG Chatbot** is a **Retrieval-Augmented Generation (RAG)** system designed specifically for **biomedical research data**.  
It retrieves relevant biomedical passages from a **FAISS vector store** and uses a **Gemma 2B model** (downloaded automatically from Hugging Face if not found locally) for generating context-aware, evidence-based responses.

The system also supports **query rewriting** to improve retrieval accuracy and avoid ambiguity in biomedical question answering.

---

## ğŸ“‚ Project Structure
```plaintext
app/
â”œâ”€â”€ embedder_model_folder/       # Local sentence-transformer model for embeddings
â”œâ”€â”€ index/
â”‚   â””â”€â”€ faiss_index_folder/      # Prebuilt FAISS vector index for biomedical corpus
â”œâ”€â”€ models/
â”‚   â””â”€â”€ gemma/                   # Local Gemma model cache (auto-downloaded if missing)
â”œâ”€â”€ __init__.py
â”œâ”€â”€ config.py                    # Loads and parses config.json
â”œâ”€â”€ gemma.py                     # Gemma model loader (auto-downloads from Hugging Face)
â”œâ”€â”€ intent.py                    # Intent-specific logic
â”œâ”€â”€ main.py                      # Flask API entry point
â”œâ”€â”€ retriever.py                 # Embedding & FAISS retrieval
â”œâ”€â”€ rewriter.py                  # Query rewriting logic
docker-entrypoint.sh             # Docker container startup script
Dockerfile                       # Docker build instructions
requirements.txt                 # Python dependencies
README.md                        # Project documentation
```

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/MukulGarg0097/Biomed-Rag-chatbot.git
cd Biomed-Rag-chatbot
```

### 2ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

---

## ğŸ“¥ Model Download (First Run)
The Gemma 2B model is automatically downloaded from Hugging Face if not already present in `app/models/gemma`.  

Since Gemma requires accepting usage terms, **you must log in to Hugging Face** and set your token:

```bash
huggingface-cli login
# OR
export HF_TOKEN=hf_xxxxxxxx
```

**Optional** â€” Change to instruction-tuned model:
```bash
export HF_MODEL_REPO=google/gemma-2-2b-it
```

---

## ğŸš€ Running the Chatbot

### Local Run
```bash
python -m app.main
```
Server starts at:
```
http://localhost:8080
```

### Docker Run
```bash
docker build -t rag_chatbot:latest .
docker run -p 8080:8080 -e HF_TOKEN=hf_xxxxx rag_chatbot:latest
```

**Run with GPU** (if available):
```bash
docker run --gpus all -p 8080:8080 -e HF_TOKEN=hf_xxxxx rag_chatbot:latest
```

---

## ğŸ›  API Endpoints

### 1. **Health Check**
```bash
curl http://localhost:8080/health
```

### 2. **Ask a Biomedical Question**
```bash
curl -X POST http://localhost:8080/ask   -H "Content-Type: application/json"   -d '{"question": "What are biomarkers for lung cancer?", "k": 3}'
```

### 3. **Reload FAISS Index**
```bash
curl -X POST http://localhost:8080/reload
```

---

## ğŸ” Query Rewriting
If `"USE_REWRITER": true` in `config.json`, the chatbot will:
1. Take your biomedical query.
2. Use Gemma to clarify terms, expand abbreviations, and remove ambiguity.
3. Send the rewritten query to FAISS for improved retrieval accuracy.

Example:
```
Original: "COVID treatment"
Rewritten: "What are the current WHO-recommended treatments for COVID-19?"
```

---

## âš ï¸ Notes
- **First run requires internet** to download Gemma; later runs can be offline:
```bash
export TRANSFORMERS_OFFLINE=true
```
- Ensure the following are available for offline use:
  - `embedder_model_folder/`
  - `index/faiss_index_folder/`

---

## ğŸ“œ License
MIT License â€” free to use and modify for research & educational purposes.
